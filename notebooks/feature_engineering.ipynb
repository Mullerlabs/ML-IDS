{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Label.csv file\n",
    "labels = pd.read_csv('../data/CICD/Label.csv')\n",
    "# Read the Data.csv file\n",
    "data = pd.read_csv('../data/CICD/Data.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in data\n",
    "missing_values = data.isnull().sum()\n",
    "\n",
    "# Display features with missing values (if any)\n",
    "print(\"Features with missing values:\")\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Calculate percentage of missing values\n",
    "missing_percentage = (data.isnull().sum() / len(data)) * 100\n",
    "\n",
    "# Display features with missing values percentage\n",
    "print(\"\\nPercentage of missing values:\")\n",
    "print(missing_percentage[missing_percentage > 0])\n",
    "\n",
    "# Get total number of missing values\n",
    "total_missing = data.isnull().sum().sum()\n",
    "print(f\"\\nTotal number of missing values: {total_missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Identify categorical-like features based on the number of unique values\n",
    "categorical_like = []\n",
    "for column in data.columns:\n",
    "    unique_values = data[column].unique()\n",
    "    num_unique = len(unique_values)\n",
    "    if num_unique < 15:  # Adjust threshold as needed\n",
    "        categorical_like.append({'column': column, 'unique_values': unique_values, 'count': num_unique})\n",
    "\n",
    "print(\"Potential categorical features:\")\n",
    "print(categorical_like)\n",
    "\n",
    "# One-Hot Encode the identified categorical features\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')  # handle_unknown to avoid errors\n",
    "\n",
    "# Extract column names from categorical_like\n",
    "categorical_columns = [item['column'] for item in categorical_like]\n",
    "\n",
    "# Fit and transform the selected columns\n",
    "encoder.fit(data[categorical_columns])\n",
    "encoded_data = encoder.transform(data[categorical_columns])\n",
    "\n",
    "# Create a new DataFrame with the encoded features\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_columns))\n",
    "\n",
    "# Concatenate the encoded DataFrame with the original DataFrame\n",
    "data = pd.concat([data, encoded_df], axis=1)\n",
    "\n",
    "# Remove original categorical columns\n",
    "data = data.drop(columns=categorical_columns)\n",
    "\n",
    "print(\"\\nDataFrame after one-hot encoding:\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a function to identify outliers based on IQR\n",
    "def find_outliers_iqr(data, threshold=1.5):\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - threshold * IQR\n",
    "    upper_bound = Q3 + threshold * IQR\n",
    "    outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "    return outliers\n",
    "\n",
    "# Identify numerical features\n",
    "numerical_features = data.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "# Calculate outlier counts for each numerical feature\n",
    "outlier_counts = {}\n",
    "for feature in numerical_features:\n",
    "    outliers = find_outliers_iqr(data[feature])\n",
    "    outlier_counts[feature] = len(outliers)\n",
    "\n",
    "# Convert the dictionary to a pandas Series for easier sorting\n",
    "outlier_counts_series = pd.Series(outlier_counts)\n",
    "\n",
    "# Sort the Series in descending order\n",
    "outlier_counts_series = outlier_counts_series.sort_values(ascending=False)\n",
    "\n",
    "# Display the top features with the most outliers\n",
    "print(\"Top features with the most outliers:\")\n",
    "print(outlier_counts_series.head(10))\n",
    "\n",
    "# Define a threshold for outlier count to remove features\n",
    "outlier_threshold = 0.05 * len(data)  # e.g., remove if more than 5% are outliers\n",
    "\n",
    "# Identify features to remove\n",
    "features_to_remove = outlier_counts_series[outlier_counts_series > outlier_threshold].index.tolist()\n",
    "\n",
    "print(\"\\nFeatures to remove due to excessive outliers:\")\n",
    "print(features_to_remove)\n",
    "\n",
    "# Remove the identified features from the DataFrame\n",
    "data = data.drop(columns=features_to_remove)\n",
    "\n",
    "print(\"\\nDataFrame after removing features with excessive outliers:\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "\n",
    "# Calculate skewness for each numerical feature\n",
    "skewness = data.apply(lambda x: skew(x))\n",
    "\n",
    "# Display skewness for each feature\n",
    "print(\"Skewness for each feature:\")\n",
    "print(skewness)\n",
    "\n",
    "# Identify highly skewed features (e.g., skewness > 5)\n",
    "highly_skewed = skewness[abs(skewness) > 5]\n",
    "print(\"\\nHighly skewed features:\")\n",
    "print(highly_skewed)\n",
    "\n",
    "# Apply log transformation to highly skewed features\n",
    "for feature in highly_skewed.index:\n",
    "    data[feature] = np.log1p(data[feature])\n",
    "\n",
    "print(\"\\nDataFrame after log transformation:\")\n",
    "print(data.head())\n",
    "\n",
    "# Display skewness after transformation\n",
    "skewness_after_log = data.apply(lambda x: skew(x))\n",
    "\n",
    "# Display skewness for each feature\n",
    "print(\"\\nSkewness after log transformation:\")\n",
    "print(skewness_after_log[highly_skewed.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Get numerical features that haven't been one-hot encoded or removed\n",
    "numerical_cols = data.select_dtypes(include=['float64']).columns.tolist()\n",
    "\n",
    "# Create StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the numerical features\n",
    "scaled_data = scaler.fit_transform(data[numerical_cols])\n",
    "\n",
    "# Convert back to DataFrame with proper column names\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=numerical_cols)\n",
    "\n",
    "# Print sample of scaled data\n",
    "print(\"Sample of scaled features:\")\n",
    "print(scaled_df.head())\n",
    "\n",
    "# Display basic statistics of scaled data\n",
    "print(\"\\nScaled data statistics:\")\n",
    "print(scaled_df.describe().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the scaled data to the original data\n",
    "data[numerical_cols] = scaled_df[numerical_cols]\n",
    "\n",
    "# Display the first few rows of the updated DataFrame\n",
    "print(\"DataFrame after scaling:\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Re-identify numerical features after previous transformations\n",
    "numerical_cols = data.select_dtypes(include=['float64']).columns.tolist()\n",
    "\n",
    "# Fit RobustScaler\n",
    "robust_scaler = RobustScaler()\n",
    "\n",
    "# Fit and transform the numerical features\n",
    "robust_scaled_data = robust_scaler.fit_transform(data[numerical_cols])\n",
    "\n",
    "# Convert back to DataFrame with proper column names\n",
    "robust_scaled_df = pd.DataFrame(robust_scaled_data, columns=numerical_cols)\n",
    "\n",
    "# Print sample of scaled data\n",
    "print(\"Sample of Robust Scaled features:\")\n",
    "print(robust_scaled_df.head())\n",
    "\n",
    "# Display basic statistics of Robust Scaled data\n",
    "print(\"\\nRobust Scaled data statistics:\")\n",
    "print(robust_scaled_df.describe().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the robust scaled data to the original data\n",
    "data[numerical_cols] = robust_scaled_df[numerical_cols]\n",
    "\n",
    "# Display the first few rows of the updated DataFrame\n",
    "print(\"DataFrame after Robust Scaling:\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Create X (features) and y (target)\n",
    "X = data\n",
    "y = labels['Label']\n",
    "\n",
    "# Create train/test split with 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "# Create and train Random Forest with class weights to handle imbalance\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': data.columns,\n",
    "    'importance': rf_classifier.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "\n",
    "# Create X (features) and y (target)\n",
    "X = data\n",
    "y = labels['Label']\n",
    "\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Apply SMOTE to training data only\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "# Grid search with cross-validation\n",
    "rf_classifier = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=3, scoring='f1_weighted', verbose=1)\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Best model\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
